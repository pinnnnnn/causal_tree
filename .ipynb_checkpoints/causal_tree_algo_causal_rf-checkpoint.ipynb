{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import pydot\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to build tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def causal_train_test_split(data, predictors, response, treatment, test_size = 0.25, estimation_size = 0.33):\n",
    "    \n",
    "    global PROP\n",
    "    PROP = 1 - estimation_size\n",
    "    \n",
    "    train_set, test_set = train_test_split(data, test_size=test_size)\n",
    "    training_sample, estimation_sample = train_test_split(train_set, test_size=estimation_size)\n",
    "    training_sample.insert(loc = 0, column = 'TRAIN_ESTIMATION_IND', value = np.ones(len(training_sample)))\n",
    "    estimation_sample.insert(loc = 0, column = 'TRAIN_ESTIMATION_IND', value = np.zeros(len(estimation_sample)))\n",
    "    new_train_set = pd.concat([training_sample, estimation_sample])\n",
    "    new_train_set = new_train_set[['TRAIN_ESTIMATION_IND'] + predictors + treatment + response]\n",
    "    test_set = test_set[predictors + treatment + response]\n",
    "    return new_train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_split(index, value, dataset):\n",
    "    \"\"\" \n",
    "    A function seperate a dataset into two numpy matrices \n",
    "    given the index of an attribute and a split value for that attribute\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "        index(int): the index of the columns of the dataset\n",
    "        value(float): the value to be compared with\n",
    "        dataset(numpy array): the dataset to split\n",
    "    \n",
    "    Output:\n",
    "    ------:\n",
    "        left(numpy array): the dataset that is split(left half)\n",
    "        right(numpy array): the dataset that is split(right half)\n",
    "    \n",
    "    \"\"\"\n",
    "    dim = dataset.shape[1]\n",
    "    left, right = np.empty(shape=[0, dim]), np.empty(shape=[0, dim])\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left = np.append(left, [row], axis = 0)\n",
    "        else:\n",
    "            right = np.append(right, [row], axis = 0)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_emse(train, est, row, index):\n",
    "    \n",
    "    # check the cardinality of the training and estimation samples, if size < *threshold*\n",
    "    # then can not do the calculation\n",
    "    train_size = len(train)\n",
    "    est_size = len(est)\n",
    "    \n",
    "    # split both training sample and estimation sample using the same rule\n",
    "    left_train, right_train = data_split(index, row[index], train)\n",
    "    left_est, right_est = data_split(index, row[index], est)\n",
    "    \n",
    "\n",
    "    ### Calculate the estimated treatment effect\n",
    "            \n",
    "    # get the cardinality of training sample for both leaves\n",
    "    left_train_size = len(left_train)\n",
    "    right_train_size = len(right_train)\n",
    "    \n",
    "    # calculate the treatment effect for both leaves, \n",
    "    left_est_response_trt = get_response(left_est, 'treatment') \n",
    "    left_est_response_ctl = get_response(left_est, 'control') \n",
    "    right_est_response_trt = get_response(right_est, 'treatment') \n",
    "    right_est_response_ctl = get_response(right_est, 'control') \n",
    "    #check cardinality of each leaf, make sure each leaf has at least *threshold* (chould be changed by user)\n",
    "    # treatment and n control to do the calculation\n",
    "    \n",
    "    left_trt_effect = left_est_response_trt.mean() - left_est_response_ctl.mean()\n",
    "    right_trt_effect = right_est_response_trt.mean() - right_est_response_ctl.mean()\n",
    "    \n",
    "    # then calculated the estimated squared treatment effect\n",
    "    e_trt_effect = (left_train_size * (left_trt_effect ** 2) + right_train_size * (right_trt_effect ** 2))/(train_size)\n",
    "    \n",
    "            \n",
    "    ### Calculate the estimated variance\n",
    "    left_var = np.var(left_est_response_trt) / PROP + np.var(left_est_response_ctl) / (1 - PROP)\n",
    "    right_var = np.var(right_est_response_trt) / PROP + np.var(right_est_response_ctl) / (1 - PROP)\n",
    "    e_var = (1 / train_size + 1 / est_size) * (left_var + right_var)\n",
    "    \n",
    "    \n",
    "    ### Calculate EMSE\n",
    "    emse = e_trt_effect - e_var    \n",
    "    \n",
    "    return emse\n",
    "    \n",
    "def get_split_emse(dataset, n_predictors):\n",
    " \n",
    "    train = dataset[dataset[:,0] == 1]\n",
    "    est = dataset[dataset[:,0] == 0]\n",
    "    \n",
    "    #get the random n predictors for each node\n",
    "    num_total_predictors = train.shape[1] - 3\n",
    "    predictor_subset_ind = random.sample(range(1, 1 + num_total_predictors), n_predictors)\n",
    "    \n",
    "\n",
    "    # initialize values to return\n",
    "    b_index, b_value, b_score, b_groups = float('inf'), float('inf'), float('-inf'), None\n",
    "    \n",
    "    for index in predictor_subset_ind:\n",
    "        for row in train:\n",
    "            groups = data_split(index, row[index], dataset)\n",
    "            emse = get_emse(train, est, row, index)\n",
    "            # if mse score gets improved (reduced actually), update the information\n",
    "            if emse > b_score:# and emse is not np.nan:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], emse, groups   \n",
    "                \n",
    "    ret_dict =  {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_response(dataset, trt):\n",
    "    if trt == 'treatment':\n",
    "        return dataset[dataset[:,-2] == 1][:,-1]\n",
    "    elif trt == 'control':\n",
    "        return dataset[dataset[:,-2] == 0][:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the split based on criterion\n",
    "def get_split(dataset, criterion, n_predictors):\n",
    "    \"\"\" \n",
    "    A function to split the data based on splitting criterion specified by user\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "        dataset(np array): a dataset in the form of a numpy matrix\n",
    "        criterion(str): a str to indicate the criterion specified by user\n",
    "    \n",
    "    Output:\n",
    "    ------:\n",
    "        the same output as functions get_split_xxx\n",
    "    \n",
    "    \"\"\"    \n",
    "    if criterion == 'mse':\n",
    "        return get_split_mse(dataset)\n",
    "    if criterion == 'causal':\n",
    "        return get_split_emse(dataset, n_predictors)    \n",
    "    elif criterion == 'gini':\n",
    "        return get_split_gini(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a terminal node value\n",
    "def to_terminal_gini(group):\n",
    "    response = group[:,-1]\n",
    "    return stats.mode(response)[0][0] # this could be optimized\n",
    "\n",
    "def to_terminal_mse(group):\n",
    "    response = group[:,-1]\n",
    "    return np.mean(response)\n",
    "\n",
    "def to_terminal_emse(group):\n",
    "    est_trt = get_response(group, 'treatment')\n",
    "    est_ctl = get_response(group, 'control')\n",
    "    \n",
    "    causal_effect = np.mean(est_trt) - np.mean(est_ctl)\n",
    "    proportion_of_data = (len(est_trt) + len(est_ctl)) / TOTAL_DATA_COUNT\n",
    "    \n",
    "    return causal_effect, round(proportion_of_data * 100, 1)\n",
    "    \n",
    "    \n",
    "def to_terminal(group, criterion):\n",
    "    if criterion == 'gini':\n",
    "        return to_terminal_gini(group)\n",
    "    elif criterion == 'mse':\n",
    "        return to_terminal_mse(group)\n",
    "    elif criterion == 'causal':\n",
    "        return to_terminal_emse(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth, criterion, n_predictors):\n",
    "    \n",
    "    left, right = node['groups']\n",
    "    \n",
    "    left_train = left[left[:,0] == 1]\n",
    "    left_est = left[left[:,0] == 0]\n",
    "    right_train = right[right[:,0] == 1]\n",
    "    right_est = right[right[:,0] == 0]    \n",
    "    \n",
    "    left_train_response_trt = get_response(left_train, 'treatment')\n",
    "    left_train_response_ctl = get_response(left_train, 'control')\n",
    "    right_train_response_trt = get_response(right_train, 'treatment')\n",
    "    right_train_response_ctl = get_response(right_train, 'control')  \n",
    "    \n",
    "    left_est_response_trt = get_response(left_est, 'treatment')\n",
    "    left_est_response_ctl = get_response(left_est, 'control')\n",
    "    right_est_response_trt = get_response(right_est, 'treatment')\n",
    "    right_est_response_ctl = get_response(right_est, 'control')\n",
    "    \n",
    "    del(node['groups'])\n",
    "    \n",
    "    if len(left) == 0 or len(right) == 0:\n",
    "        node['left'] = node['right'] = to_terminal(np.append(left, right, axis = 0), criterion)\n",
    "        return\n",
    "    \n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left, criterion), to_terminal(right, criterion)\n",
    "        return\n",
    "    \n",
    "    # process left child\n",
    "    if (len(left) <= min_size or len(left_est_response_trt) <= min_size or len(left_est_response_ctl) <= min_size or\n",
    "        len(right_est_response_trt) <= min_size or len(right_est_response_ctl) <= min_size or \n",
    "        len(left_train_response_trt) <= min_size or len(left_train_response_ctl) <= min_size or \n",
    "        len(right_train_response_trt) <= min_size or len(right_train_response_ctl) <= min_size):\n",
    "        node['left'] = to_terminal(left, criterion)\n",
    "    else:\n",
    "        node['left'] = get_split(left, criterion, n_predictors)\n",
    "        if node['left']['groups'] is None:\n",
    "            node['left'] = to_terminal(left, criterion)\n",
    "        else:\n",
    "            split(node['left'], max_depth, min_size, depth+1, criterion, n_predictors)\n",
    "        \n",
    "    # process right child\n",
    "    if (len(right) <= min_size or len(left_est_response_trt) <= min_size or len(left_est_response_ctl) <= min_size or\n",
    "        len(right_est_response_trt) <= min_size or len(right_est_response_ctl) <= min_size or \n",
    "        len(left_train_response_trt) <= min_size or len(left_train_response_ctl) <= min_size or \n",
    "        len(right_train_response_trt) <= min_size or len(right_train_response_ctl) <= min_size):\n",
    "        node['right'] = to_terminal(right, criterion)\n",
    "    else:\n",
    "        node['right'] = get_split(right, criterion, n_predictors)\n",
    "        if node['right']['groups'] is None:\n",
    "            node['right'] = to_terminal(right, criterion)\n",
    "        else:\n",
    "            split(node['right'], max_depth, min_size, depth+1, criterion, n_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a decision tree\n",
    "def causalTree(train, max_depth, min_size, criterion, n_predictors):\n",
    "    \n",
    "    global TOTAL_DATA_COUNT, COLUMN_NAMES\n",
    "    TOTAL_DATA_COUNT = len(train)\n",
    "    COLUMN_NAMES = train.columns[1:-2]\n",
    "    \n",
    "    train = np.array(train)\n",
    "    \n",
    "    if criterion == 'mse' or criterion == 'gini':\n",
    "        root = get_split(train, criterion)\n",
    "        #print(root)\n",
    "        split(root, max_depth, min_size, 1, criterion)\n",
    "        \n",
    "    elif criterion == 'causal':\n",
    "        root = get_split(train, criterion, n_predictors)\n",
    "        split(root, max_depth, min_size, 1, criterion, n_predictors)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to use causal tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print out the tree structure\n",
    "def print_tree(node, depth=0):\n",
    "    if isinstance(node, dict):\n",
    "        print('%s[%s < %.3f]' % ((depth * ' ', (COLUMN_NAMES[node['index'] - 1]), node['value'])))\n",
    "        print_tree(node['left'], depth + 1)\n",
    "        print_tree(node['right'], depth + 1)\n",
    "    else:\n",
    "        print('%s[%s, %s%%]' % ((depth * ' ', node[0], node[1])))\n",
    "\n",
    "        \n",
    "#causal effect prediction\n",
    "def causalPredict_helper(node,row):\n",
    "    if row[node['index'] - 1] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return causalPredict_helper(node['left'], row)\n",
    "        else:\n",
    "            return node['left'][0]\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return causalPredict_helper(node['right'], row)\n",
    "        else:\n",
    "            return node['right'][0]    \n",
    "            \n",
    "def causalPredict(test, tree):\n",
    "    #get the information of the trainning set and initialize an empty return dataframe\n",
    "    column_names = list(test.columns) + ['pred_causal_effect']\n",
    "    test_matrix = np.array(test)\n",
    "    ret_matrix = np.empty([0, test_matrix.shape[1] + 1])\n",
    "    \n",
    "    #predict for each row\n",
    "    for row in test_matrix:\n",
    "        row = np.insert(row, len(row), causalPredict_helper(tree, row))\n",
    "        ret_matrix = np.append(ret_matrix, [row], axis = 0) \n",
    "    \n",
    "    #return a new dataframe with the predicted value at the end of each row\n",
    "    ret_df = pd.DataFrame(ret_matrix, columns = column_names)\n",
    "\n",
    "    return ret_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rf_sample(train_set, sample_ratio, n_predictors):\n",
    "    ###create a new random forest training set\n",
    "    \n",
    "#     #get subset of predictors based on given num of predictors\n",
    "#     num_total_predictors = len(train_set.columns) - 3\n",
    "#     predictor_subset_ind = random.sample(range(1, 1 + num_total_predictors), n_predictors)\n",
    "#     rf_columns = ['TRAIN_ESTIMATION_IND'] + list(train_set.columns[predictor_subset_ind]) + ['trt', 'y']\n",
    "#     train_subset = train_set[rf_columns]\n",
    "    \n",
    "    #get a subset of rows based on given sample ratio with replacement/bootsrap\n",
    "    n_row = int(sample_ratio * len(train_set))\n",
    "    new_train_subset = train_set.sample(n = n_row, replace = True, axis = 0)\n",
    "    \n",
    "    return new_train_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def causalRandomForest(train_set, test_set, max_depth, min_size, criterion, sample_ratio, n_predictors, n_trees):\n",
    "    \n",
    "    n_test_set = len(test_set)\n",
    "    pred_value_list = np.empty([1,n_test_set])\n",
    "    p_value_list = np.empty([1,n_test_set])\n",
    "    \n",
    "    for i in range(n_trees):\n",
    "        #get rf sub training data\n",
    "        rf_train_set = rf_sample(train_set, sample_ratio, n_predictors)\n",
    "        #build the tree\n",
    "        tree = causalTree(rf_train_set, max_depth, min_size, criterion, n_predictors)\n",
    "        \n",
    "        # get the prediction for each tree\n",
    "        pred_df = causalPredict(test_set, tree)\n",
    "        pred_values = np.array(pred_df['pred_causal_effect'])\n",
    "        # aggregate the prediction\n",
    "        pred_value_list = pred_value_list + pred_values\n",
    "        \n",
    "        # get the number of negative causal effect in the prediction\n",
    "        pred_values[pred_values >= 0] = 0 # set positive causal effect to 0\n",
    "        pred_values[pred_values < 0] = 1 # set negative causal effect to 1\n",
    "        p_value_list = p_value_list + pred_values\n",
    "        \n",
    "    #calculate the prediction of bagged trees for each data point\n",
    "    pred_value_rf = pred_value_list / n_trees\n",
    "    #calculate the p value of bagged trees for each data point\n",
    "    p_value_rf = p_value_list / n_trees\n",
    "    \n",
    "    #append the prediction and p_value to the dataset\n",
    "    ret_data = test_set.copy()\n",
    "    ret_data['rf_pred_causal_effect'] = pred_value_rf[0]\n",
    "    ret_data['rf_p_value'] = p_value_rf[0]\n",
    "    \n",
    "    return ret_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random forest test\n",
    "rf_df = pd.read_csv('fake_data_large.csv')\n",
    "\n",
    "#get the column names of predictors\n",
    "p_str = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10']\n",
    "#get the column name of response\n",
    "r_str = ['design1_y']\n",
    "#get the column name of treatment\n",
    "t_str = ['treatment']\n",
    "\n",
    "#set a random seed for replication \n",
    "np.random.seed(42)\n",
    "\n",
    "#split the data\n",
    "rf_train_set, rf_test_set = causal_train_test_split(rf_df, predictors = p_str, response = r_str, treatment = t_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>treatment</th>\n",
       "      <th>design1_y</th>\n",
       "      <th>rf_pred_causal_effect</th>\n",
       "      <th>rf_p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>-0.087134</td>\n",
       "      <td>0.056415</td>\n",
       "      <td>0.022938</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>-0.295644</td>\n",
       "      <td>1.363628</td>\n",
       "      <td>0.632985</td>\n",
       "      <td>0.025216</td>\n",
       "      <td>0.396586</td>\n",
       "      <td>1.413206</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000815</td>\n",
       "      <td>-0.014849</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>-0.007295</td>\n",
       "      <td>-0.127284</td>\n",
       "      <td>1.906676</td>\n",
       "      <td>0.173164</td>\n",
       "      <td>2.215678</td>\n",
       "      <td>0.949329</td>\n",
       "      <td>0.617670</td>\n",
       "      <td>0.012248</td>\n",
       "      <td>0.987231</td>\n",
       "      <td>0.556778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.126840</td>\n",
       "      <td>-0.013333</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>-0.034110</td>\n",
       "      <td>0.383475</td>\n",
       "      <td>0.374740</td>\n",
       "      <td>0.146836</td>\n",
       "      <td>4.125315</td>\n",
       "      <td>1.614067</td>\n",
       "      <td>0.296285</td>\n",
       "      <td>0.095118</td>\n",
       "      <td>0.033616</td>\n",
       "      <td>1.375010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.348085</td>\n",
       "      <td>-0.004416</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>-0.152610</td>\n",
       "      <td>0.073485</td>\n",
       "      <td>3.255523</td>\n",
       "      <td>0.358538</td>\n",
       "      <td>4.383789</td>\n",
       "      <td>0.102463</td>\n",
       "      <td>0.276884</td>\n",
       "      <td>0.158907</td>\n",
       "      <td>0.918488</td>\n",
       "      <td>1.355233</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.053169</td>\n",
       "      <td>-0.080723</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0.039056</td>\n",
       "      <td>0.299605</td>\n",
       "      <td>0.342748</td>\n",
       "      <td>0.183995</td>\n",
       "      <td>1.479058</td>\n",
       "      <td>2.109863</td>\n",
       "      <td>0.616161</td>\n",
       "      <td>0.296946</td>\n",
       "      <td>0.004435</td>\n",
       "      <td>2.763050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315944</td>\n",
       "      <td>-0.019011</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2        x3        x4        x5        x6        x7  \\\n",
       "521 -0.087134  0.056415  0.022938  0.105300 -0.295644  1.363628  0.632985   \n",
       "737 -0.007295 -0.127284  1.906676  0.173164  2.215678  0.949329  0.617670   \n",
       "740 -0.034110  0.383475  0.374740  0.146836  4.125315  1.614067  0.296285   \n",
       "660 -0.152610  0.073485  3.255523  0.358538  4.383789  0.102463  0.276884   \n",
       "411  0.039056  0.299605  0.342748  0.183995  1.479058  2.109863  0.616161   \n",
       "\n",
       "           x8        x9       x10  treatment  design1_y  \\\n",
       "521  0.025216  0.396586  1.413206        1.0  -0.000815   \n",
       "737  0.012248  0.987231  0.556778        1.0  -0.126840   \n",
       "740  0.095118  0.033616  1.375010        1.0   0.348085   \n",
       "660  0.158907  0.918488  1.355233        1.0  -0.053169   \n",
       "411  0.296946  0.004435  2.763050        0.0   0.315944   \n",
       "\n",
       "     rf_pred_causal_effect  rf_p_value  \n",
       "521              -0.014849         0.8  \n",
       "737              -0.013333         0.8  \n",
       "740              -0.004416         0.6  \n",
       "660              -0.080723         0.9  \n",
       "411              -0.019011         0.7  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run rf to get predicted causal effect\n",
    "rf_emse_1 = causalRandomForest(rf_train_set, rf_test_set, max_depth = 3, min_size = 10, \n",
    "                          criterion = 'causal', sample_ratio = 0.8, n_predictors = 6, n_trees = 10)\n",
    "rf_emse_1.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
