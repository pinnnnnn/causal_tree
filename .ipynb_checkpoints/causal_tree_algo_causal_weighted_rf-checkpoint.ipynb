{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pydot\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to build tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def causal_train_test_split(data, predictors, response, treatment, test_size = 0.25, estimation_size = 0.33):\n",
    "    \n",
    "    global PROP\n",
    "    PROP = 1 - estimation_size\n",
    "    \n",
    "    train_set, test_set = train_test_split(data, test_size=test_size)\n",
    "    training_sample, estimation_sample = train_test_split(train_set, test_size=estimation_size)\n",
    "    training_sample.insert(loc = 0, column = 'TRAIN_ESTIMATION_IND', value = np.ones(len(training_sample)))\n",
    "    estimation_sample.insert(loc = 0, column = 'TRAIN_ESTIMATION_IND', value = np.zeros(len(estimation_sample)))\n",
    "    new_train_set = pd.concat([training_sample, estimation_sample])\n",
    "    new_train_set = new_train_set[['TRAIN_ESTIMATION_IND'] + predictors + treatment + response]\n",
    "    test_set = test_set[predictors + treatment + response]\n",
    "    return new_train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_split(index, value, dataset):\n",
    "    \"\"\" \n",
    "    A function seperate a dataset into two numpy matrices \n",
    "    given the index of an attribute and a split value for that attribute\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "        index(int): the index of the columns of the dataset\n",
    "        value(float): the value to be compared with\n",
    "        dataset(numpy array): the dataset to split\n",
    "    \n",
    "    Output:\n",
    "    ------:\n",
    "        left(numpy array): the dataset that is split(left half)\n",
    "        right(numpy array): the dataset that is split(right half)\n",
    "    \n",
    "    \"\"\"\n",
    "    dim = dataset.shape[1]\n",
    "    left, right = np.empty(shape=[0, dim]), np.empty(shape=[0, dim])\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left = np.append(left, [row], axis = 0)\n",
    "        else:\n",
    "            right = np.append(right, [row], axis = 0)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_emse(train, est, row, index):\n",
    "    \n",
    "    # check the cardinality of the training and estimation samples, if size < *threshold*\n",
    "    # then can not do the calculation\n",
    "    train_size = len(train)\n",
    "    est_size = len(est)\n",
    "    \n",
    "    # split both training sample and estimation sample using the same rule\n",
    "    left_train, right_train = data_split(index, row[index], train)\n",
    "    left_est, right_est = data_split(index, row[index], est)\n",
    "    \n",
    "\n",
    "    ### Calculate the estimated treatment effect\n",
    "            \n",
    "    # get the cardinality of training sample for both leaves\n",
    "    left_train_size = len(left_train)\n",
    "    right_train_size = len(right_train)\n",
    "    \n",
    "    # calculate the treatment effect for both leaves, \n",
    "    left_est_response_trt = get_response(left_est, 'treatment') \n",
    "    left_est_response_ctl = get_response(left_est, 'control') \n",
    "    right_est_response_trt = get_response(right_est, 'treatment') \n",
    "    right_est_response_ctl = get_response(right_est, 'control') \n",
    "    #check cardinality of each leaf, make sure each leaf has at least *threshold* (chould be changed by user)\n",
    "    # treatment and n control to do the calculation\n",
    "    \n",
    "    left_trt_effect = left_est_response_trt.mean() - left_est_response_ctl.mean()\n",
    "    right_trt_effect = right_est_response_trt.mean() - right_est_response_ctl.mean()\n",
    "    \n",
    "    # then calculated the estimated squared treatment effect\n",
    "    e_trt_effect = (left_train_size * (left_trt_effect ** 2) + right_train_size * (right_trt_effect ** 2))/(train_size)\n",
    "    \n",
    "            \n",
    "    ### Calculate the estimated variance\n",
    "    left_var = np.var(left_est_response_trt) / PROP + np.var(left_est_response_ctl) / (1 - PROP)\n",
    "    right_var = np.var(right_est_response_trt) / PROP + np.var(right_est_response_ctl) / (1 - PROP)\n",
    "    e_var = (1 / train_size + 1 / est_size) * (left_var + right_var)\n",
    "    \n",
    "    \n",
    "    ### Calculate EMSE\n",
    "    emse = e_trt_effect - e_var    \n",
    "    \n",
    "    return emse\n",
    "    \n",
    "def get_split_emse(dataset):\n",
    " \n",
    "    train = dataset[dataset[:,0] == 1]\n",
    "    est = dataset[dataset[:,0] == 0]\n",
    "\n",
    "    # initialize values to return\n",
    "    b_index, b_value, b_score, b_groups = float('inf'), float('inf'), float('-inf'), None\n",
    "    \n",
    "    #get the new set of predictors according to PRED_WEIGHT\n",
    "    pred_lst = sample_by_weight(PRED_WEIGHT, COLUMN_NAMES)\n",
    "    pred_lst_unique = list(set(pred_lst))\n",
    "    pred_index_lst = [COLUMN_NAMES.index(x) for x in pred_lst_unique]\n",
    "    \n",
    "    \n",
    "    for index in pred_index_lst:\n",
    "        for row in train:\n",
    "            groups = data_split(index, row[index], dataset)\n",
    "            emse = get_emse(train, est, row, index)\n",
    "            # if mse score gets improved (reduced actually), update the information\n",
    "            if emse > b_score:# and emse is not np.nan:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], emse, groups   \n",
    "                \n",
    "    ret_dict =  {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_response(dataset, trt):\n",
    "    if trt == 'treatment':\n",
    "        return dataset[dataset[:,-2] == 1][:,-1]\n",
    "    elif trt == 'control':\n",
    "        return dataset[dataset[:,-2] == 0][:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the split based on criterion\n",
    "def get_split(dataset, criterion):\n",
    "    \"\"\" \n",
    "    A function to split the data based on splitting criterion specified by user\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "        dataset(np array): a dataset in the form of a numpy matrix\n",
    "        criterion(str): a str to indicate the criterion specified by user\n",
    "    \n",
    "    Output:\n",
    "    ------:\n",
    "        the same output as functions get_split_xxx\n",
    "    \n",
    "    \"\"\"    \n",
    "    if criterion == 'mse':\n",
    "        return get_split_mse(dataset)\n",
    "    if criterion == 'causal':\n",
    "        return get_split_emse(dataset)    \n",
    "    elif criterion == 'gini':\n",
    "        return get_split_gini(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a terminal node value\n",
    "def to_terminal_gini(group):\n",
    "    response = group[:,-1]\n",
    "    return stats.mode(response)[0][0] # this could be optimized\n",
    "\n",
    "def to_terminal_mse(group):\n",
    "    response = group[:,-1]\n",
    "    return np.mean(response)\n",
    "\n",
    "def to_terminal_emse(group):\n",
    "    est_trt = get_response(group, 'treatment')\n",
    "    est_ctl = get_response(group, 'control')\n",
    "    \n",
    "    causal_effect = np.mean(est_trt) - np.mean(est_ctl)\n",
    "    proportion_of_data = (len(est_trt) + len(est_ctl)) / TOTAL_DATA_COUNT\n",
    "    \n",
    "    return causal_effect, round(proportion_of_data * 100, 1)\n",
    "    \n",
    "    \n",
    "def to_terminal(group, criterion):\n",
    "    if criterion == 'gini':\n",
    "        return to_terminal_gini(group)\n",
    "    elif criterion == 'mse':\n",
    "        return to_terminal_mse(group)\n",
    "    elif criterion == 'causal':\n",
    "        return to_terminal_emse(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth, criterion):\n",
    "    \n",
    "    left, right = node['groups']\n",
    "    \n",
    "    left_train = left[left[:,0] == 1]\n",
    "    left_est = left[left[:,0] == 0]\n",
    "    right_train = right[right[:,0] == 1]\n",
    "    right_est = right[right[:,0] == 0]    \n",
    "    \n",
    "    left_train_response_trt = get_response(left_train, 'treatment')\n",
    "    left_train_response_ctl = get_response(left_train, 'control')\n",
    "    right_train_response_trt = get_response(right_train, 'treatment')\n",
    "    right_train_response_ctl = get_response(right_train, 'control')  \n",
    "    \n",
    "    left_est_response_trt = get_response(left_est, 'treatment')\n",
    "    left_est_response_ctl = get_response(left_est, 'control')\n",
    "    right_est_response_trt = get_response(right_est, 'treatment')\n",
    "    right_est_response_ctl = get_response(right_est, 'control')\n",
    "    \n",
    "    del(node['groups'])\n",
    "    \n",
    "    if len(left) == 0 or len(right) == 0:\n",
    "        node['left'] = node['right'] = to_terminal(np.append(left, right, axis = 0), criterion)\n",
    "        return\n",
    "    \n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left, criterion), to_terminal(right, criterion)\n",
    "        return\n",
    "    \n",
    "    # process left child\n",
    "    if (len(left) <= min_size or len(left_est_response_trt) <= min_size or len(left_est_response_ctl) <= min_size or\n",
    "        len(right_est_response_trt) <= min_size or len(right_est_response_ctl) <= min_size or \n",
    "        len(left_train_response_trt) <= min_size or len(left_train_response_ctl) <= min_size or \n",
    "        len(right_train_response_trt) <= min_size or len(right_train_response_ctl) <= min_size):\n",
    "        node['left'] = to_terminal(left, criterion)\n",
    "    else:\n",
    "        node['left'] = get_split(left, criterion)\n",
    "        if node['left']['groups'] is None:\n",
    "            node['left'] = to_terminal(left, criterion)\n",
    "        else:\n",
    "            split(node['left'], max_depth, min_size, depth+1, criterion)\n",
    "        \n",
    "    # process right child\n",
    "    if (len(right) <= min_size or len(left_est_response_trt) <= min_size or len(left_est_response_ctl) <= min_size or\n",
    "        len(right_est_response_trt) <= min_size or len(right_est_response_ctl) <= min_size or \n",
    "        len(left_train_response_trt) <= min_size or len(left_train_response_ctl) <= min_size or \n",
    "        len(right_train_response_trt) <= min_size or len(right_train_response_ctl) <= min_size):\n",
    "        node['right'] = to_terminal(right, criterion)\n",
    "    else:\n",
    "        node['right'] = get_split(right, criterion)\n",
    "        if node['right']['groups'] is None:\n",
    "            node['right'] = to_terminal(right, criterion)\n",
    "        else:\n",
    "            split(node['right'], max_depth, min_size, depth+1, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a decision tree\n",
    "def causalTree(train, max_depth, min_size, criterion):\n",
    "    \n",
    "    global TOTAL_DATA_COUNT, COLUMN_NAMES\n",
    "    TOTAL_DATA_COUNT = len(train)\n",
    "    COLUMN_NAMES = list(train.columns[1:-2])\n",
    "    \n",
    "    train = np.array(train)\n",
    "    \n",
    "#     if criterion == 'mse' or criterion == 'gini':\n",
    "#         root = get_split(train, criterion)\n",
    "#         #print(root)\n",
    "#         split(root, max_depth, min_size, 1, criterion)\n",
    "        \n",
    "    if criterion == 'causal':\n",
    "        root = get_split(train, criterion)\n",
    "        split(root, max_depth, min_size, 1, criterion)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to use causal tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print out the tree structure\n",
    "def print_tree(node, depth=0):\n",
    "    if isinstance(node, dict):\n",
    "        print('%s[%s < %.3f]' % ((depth * ' ', ([node['index'] - 1]), node['value'])))\n",
    "        print_tree(node['left'], depth + 1)\n",
    "        print_tree(node['right'], depth + 1)\n",
    "    else:\n",
    "        print('%s[%s, %s%%]' % ((depth * ' ', node[0], node[1])))\n",
    "\n",
    "        \n",
    "#causal effect prediction\n",
    "def causalPredict_helper(node,row):\n",
    "    if row[node['index'] - 1] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return causalPredict_helper(node['left'], row)\n",
    "        else:\n",
    "            return node['left'][0]\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return causalPredict_helper(node['right'], row)\n",
    "        else:\n",
    "            return node['right'][0]    \n",
    "            \n",
    "def causalPredict(test, tree):\n",
    "    #get the information of the trainning set and initialize an empty return dataframe\n",
    "    column_names = list(test.columns) + ['pred_causal_effect']\n",
    "    test_matrix = np.array(test)\n",
    "    ret_matrix = np.empty([0, test_matrix.shape[1] + 1])\n",
    "    \n",
    "    #predict for each row\n",
    "    for row in test_matrix:\n",
    "        row = np.insert(row, len(row), causalPredict_helper(tree, row))\n",
    "        ret_matrix = np.append(ret_matrix, [row], axis = 0) \n",
    "    \n",
    "    #return a new dataframe with the predicted value at the end of each row\n",
    "    ret_df = pd.DataFrame(ret_matrix, columns = column_names)\n",
    "\n",
    "    return ret_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rf_sample(train_set, sample_ratio):\n",
    "    \"\"\"\n",
    "    a helper function to create a new random forest training set\n",
    "    \"\"\"\n",
    "    #get a subset of rows based on given sample ratio with replacement/bootsrap\n",
    "    n_row = int(sample_ratio * len(train_set))\n",
    "    new_train_subset = train_set.sample(n = n_row, replace = True, axis = 0)\n",
    "    \n",
    "    return new_train_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to build a random forest predictor\n",
    "def causalRandomForestWeighted(train_set, max_depth, min_size, criterion, sample_ratio, n_trees):\n",
    "    tree_lst = []\n",
    "    for i in range(n_trees):\n",
    "        #get rf sub training data\n",
    "        rf_train_set = rf_sample(train_set, sample_ratio)\n",
    "        #build the tree\n",
    "        tree = causalTree(rf_train_set, max_depth, min_size, criterion)    \n",
    "        tree_lst.append(tree)\n",
    "        \n",
    "    return tree_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictTestSet(rf, test_set):\n",
    "       \n",
    "    n_trees = len(rf)\n",
    "    n_test_set = len(test_set)\n",
    "    pred_value_list = np.zeros([1,n_test_set])\n",
    "    p_value_list = np.zeros([1,n_test_set])  \n",
    "    \n",
    "    for tree in rf:\n",
    "        # get the prediction for each tree\n",
    "        pred_df = causalPredict(test_set, tree)\n",
    "        pred_values = np.array(pred_df['pred_causal_effect'])\n",
    "        # aggregate the prediction\n",
    "        pred_value_list = pred_value_list + pred_values\n",
    "        \n",
    "        # get the number of negative causal effect in the prediction\n",
    "        pred_values[pred_values >= 0] = 0 # set positive causal effect to 0\n",
    "        pred_values[pred_values < 0] = 1 # set negative causal effect to 1\n",
    "        p_value_list = p_value_list + pred_values\n",
    "        \n",
    "    #calculate the prediction of bagged trees for each data point\n",
    "    pred_value_rf = pred_value_list / n_trees #np.round_(pred_value_list / n_trees, decimals = 3)\n",
    "    #calculate the p value of bagged trees for each data point\n",
    "    p_value_rf = p_value_list / n_trees #np.round_(p_value_list / n_trees, decimals = 3)\n",
    "    \n",
    "    #append the prediction and p_value to the dataset\n",
    "    ret_data = test_set.copy()\n",
    "    ret_data['rf_pred_causal_effect'] = pred_value_rf[0]\n",
    "    # calculate the p value against the hypothesis that the causal effect is not 0\n",
    "    ret_data['rf_p_value'] = p_value_rf[0]\n",
    "    ret_data['rf_p_value'] = ret_data['rf_p_value'].apply(lambda row: 2*row if row < 0.5 else 2*(1-row))\n",
    "    \n",
    "    return ret_data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictDataPoint(rf, data):\n",
    "    test_set = pd.DataFrame([data], columns = data.keys())\n",
    "    predict_list = []\n",
    "    for tree in rf:\n",
    "        pred_df = causalPredict(test_set, tree)\n",
    "        pred_value = pred_df.iloc[0,-1]\n",
    "        predict_list.append(pred_value)\n",
    "    # calculate the predicted value    \n",
    "    rf_pred_value = round(np.mean(predict_list),3)\n",
    "    # calculate p-value\n",
    "    positive_ratio = sum(x >0 for x in predict_list)/len(predict_list)\n",
    "    if positive_ratio <= 0.5:\n",
    "        p_value = 2*positive_ratio\n",
    "    else:\n",
    "        p_value = 2*(1-positive_ratio)\n",
    "    print(\"Predicted causal effect: \" + str(rf_pred_value) + \"\\n\")\n",
    "    print(\"P-value: \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weighted random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mse(df, pred_col, true_col):\n",
    "    \"\"\"\n",
    "    compute the mse given a dataframe and its corresponding predicted column name and true column name\n",
    "    \"\"\"\n",
    "    mse = np.mean((df[pred_col] - df[true_col]) ** 2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_by_weight(p_weight, p_str):\n",
    "    \"\"\"\n",
    "    sample a random n predictors with replacement by the weight of each predictors\n",
    "    \"\"\"\n",
    "    if len(p_weight) != len(p_str):\n",
    "        raise ValueError('The length of the predictor list and the weight list must be the same')\n",
    "    \n",
    "    #get the probability list of each predictors\n",
    "    prob_lst = [x/sum(p_weight) for x in p_weight]\n",
    "    \n",
    "    #randomly draw a sample (indices) according to the probability distribution\n",
    "    index_lst = []\n",
    "    for i in range(len(prob_lst)):\n",
    "        index_lst.append(np.random.choice(np.arange(0, len(prob_lst)), p = prob_lst))\n",
    "        \n",
    "    #get the predictors names from the indices\n",
    "    sample_pred_lst = [p_str[i] for i in index_lst]\n",
    "    \n",
    "    return sample_pred_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle_column(df, col_name):\n",
    "    \"\"\"\n",
    "    shuffle a column of a dataframe\n",
    "    \"\"\"\n",
    "    ret_df = df.copy()\n",
    "    shuffle_lst = np.array(ret_df[col_name])\n",
    "    np.random.shuffle(shuffle_lst)\n",
    "    ret_df[col_name] = shuffle_lst\n",
    "    return ret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>treatment</th>\n",
       "      <th>error</th>\n",
       "      <th>true_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.571753</td>\n",
       "      <td>-0.348377</td>\n",
       "      <td>1.591885</td>\n",
       "      <td>-0.421881</td>\n",
       "      <td>-0.623928</td>\n",
       "      <td>1.967688</td>\n",
       "      <td>0.541506</td>\n",
       "      <td>-0.118146</td>\n",
       "      <td>2.823623</td>\n",
       "      <td>0.253072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.007519</td>\n",
       "      <td>0.785877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.710885</td>\n",
       "      <td>-0.853989</td>\n",
       "      <td>1.080298</td>\n",
       "      <td>0.686010</td>\n",
       "      <td>0.045241</td>\n",
       "      <td>2.196342</td>\n",
       "      <td>-0.128070</td>\n",
       "      <td>-1.004549</td>\n",
       "      <td>0.955377</td>\n",
       "      <td>1.731303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>-0.855443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-2.185188</td>\n",
       "      <td>0.673428</td>\n",
       "      <td>0.935457</td>\n",
       "      <td>-0.102679</td>\n",
       "      <td>1.147379</td>\n",
       "      <td>-1.310356</td>\n",
       "      <td>0.520439</td>\n",
       "      <td>0.259379</td>\n",
       "      <td>0.507608</td>\n",
       "      <td>3.954316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002132</td>\n",
       "      <td>-1.092594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.410518</td>\n",
       "      <td>-1.022942</td>\n",
       "      <td>0.968002</td>\n",
       "      <td>2.305879</td>\n",
       "      <td>0.824120</td>\n",
       "      <td>3.345610</td>\n",
       "      <td>1.631188</td>\n",
       "      <td>-0.736464</td>\n",
       "      <td>2.196028</td>\n",
       "      <td>1.552748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.205259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.755752</td>\n",
       "      <td>0.117014</td>\n",
       "      <td>1.223863</td>\n",
       "      <td>0.042935</td>\n",
       "      <td>-0.101483</td>\n",
       "      <td>-0.643004</td>\n",
       "      <td>1.368282</td>\n",
       "      <td>-1.804749</td>\n",
       "      <td>1.627001</td>\n",
       "      <td>1.371938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003179</td>\n",
       "      <td>0.877876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        x1        x2        x3        x4        x5        x6  \\\n",
       "0           0  1.571753 -0.348377  1.591885 -0.421881 -0.623928  1.967688   \n",
       "1           1 -1.710885 -0.853989  1.080298  0.686010  0.045241  2.196342   \n",
       "2           2 -2.185188  0.673428  0.935457 -0.102679  1.147379 -1.310356   \n",
       "3           3  0.410518 -1.022942  0.968002  2.305879  0.824120  3.345610   \n",
       "4           4  1.755752  0.117014  1.223863  0.042935 -0.101483 -0.643004   \n",
       "\n",
       "         x7        x8        x9       x10  treatment     error  true_val  \n",
       "0  0.541506 -0.118146  2.823623  0.253072        0.0 -0.007519  0.785877  \n",
       "1 -0.128070 -1.004549  0.955377  1.731303        0.0 -0.000603 -0.855443  \n",
       "2  0.520439  0.259379  0.507608  3.954316        0.0 -0.002132 -1.092594  \n",
       "3  1.631188 -0.736464  2.196028  1.552748        0.0  0.001370  0.205259  \n",
       "4  1.368282 -1.804749  1.627001  1.371938        0.0 -0.003179  0.877876  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_weighted_rf(train_set, test_set, n_iter, true_col_name, max_depth, min_size, criterion, sample_ratio, n_trees):\n",
    "    \"\"\"\n",
    "    rf: a trained random forest\n",
    "    test_set: test dataset\n",
    "    n_iter: number of iterations\n",
    "    p_str: a list of predictor names\n",
    "    true_col_name(str): column name of the true value\n",
    "    \n",
    "    A function to iteratively get weight on each predictors\n",
    "    \"\"\"\n",
    "    \n",
    "    #initiate a weight list(equal weight for each predictor to start with)\n",
    "    global PRED_WEIGHT\n",
    "    PRED_WEIGHT = [1/len(p_str)] * len(p_str)\n",
    "    \n",
    "    for j in range(n_iter):\n",
    "        #print(j)\n",
    "        #build the initiate random forest and compute full mse\n",
    "        rf = causalRandomForestWeighted(train_set, max_depth, min_size, criterion, sample_ratio, n_trees)\n",
    "        pred_df = predictTestSet(rf, test_set)\n",
    "        full_mse = get_mse(pred_df, 'rf_pred_causal_effect', true_col_name)\n",
    "\n",
    "        #for the test set, shuffle each column and compute the different in mse, get the difference as weight\n",
    "        col_names = list(train_set.columns[1:-2])\n",
    "        \n",
    "        #print(full_mse)\n",
    "        \n",
    "        for i in range(len(col_names)):\n",
    "            col = col_names[i]\n",
    "            test_set_shuffled = shuffle_column(test_set, col)\n",
    "            pred_df_shuffled = predictTestSet(rf, test_set_shuffled)\n",
    "            mse_shuffled = get_mse(pred_df_shuffled, 'rf_pred_causal_effect', true_col_name)\n",
    "            #print(mse_shuffled)\n",
    "            PRED_WEIGHT[i] = abs(mse_shuffled - full_mse)\n",
    "            #print(PRED_WEIGHT)\n",
    "    \n",
    "    ret_df = pd.DataFrame()\n",
    "    ret_df['Column'] = col_names\n",
    "    ret_df['Weight'] = PRED_WEIGHT\n",
    "    \n",
    "    return ret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"simulation//simulation_study5_large_sample_unif//large_unif.csv\")\n",
    "data['true_val'] = data['x1']/2\n",
    "#take a subset of data\n",
    "data1 = data.loc[0:200,:]\n",
    "data2 = data.loc[4000:4200,:]\n",
    "data = data1.append(data2)\n",
    "\n",
    "# random forest test\n",
    "#get the column names of predictors\n",
    "p_str = ['x1', 'x2','x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10']\n",
    "#get the column name of response\n",
    "r_str = ['true_val']\n",
    "#get the column name of treatment\n",
    "t_str = ['treatment']\n",
    "\n",
    "rf_train_set, rf_test_set = causal_train_test_split(data, predictors = p_str, response = r_str, treatment = t_str)\n",
    "#rf = causalRandomForest(rf_train_set, max_depth = 3, min_size = 10, \n",
    "#                        criterion = 'causal', sample_ratio = 0.8, n_trees = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3758071035793806\n",
      "1.3758071035793806\n",
      "1.3711705697991527\n",
      "1.3758071035793806\n",
      "1.38212889362404\n",
      "1.3577521289371908\n",
      "1.3758071035793806\n",
      "1.3758071035793806\n",
      "1.374306601957993\n",
      "1.3720363199260994\n",
      "1.3758071035793806\n",
      "1.36042036436754\n",
      "1.397060391721732\n",
      "1.36042036436754\n",
      "1.3600833286732434\n",
      "1.3552094191574473\n",
      "1.36042036436754\n",
      "1.36042036436754\n",
      "1.3590903678019168\n",
      "1.3455018145728264\n",
      "1.36042036436754\n",
      "1.36042036436754\n"
     ]
    }
   ],
   "source": [
    "weight_df = get_weighted_rf(rf_train_set, rf_test_set, n_iter = 2, true_col_name = 'true_val', \n",
    "                max_depth = 3, min_size = 10, criterion ='causal', sample_ratio = 0.8, n_trees = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x1</td>\n",
       "      <td>0.036640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x3</td>\n",
       "      <td>0.000337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x4</td>\n",
       "      <td>0.005211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>x6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>x7</td>\n",
       "      <td>0.001330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>x8</td>\n",
       "      <td>0.014919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>x9</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>x10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Column    Weight\n",
       "0     x1  0.036640\n",
       "1     x2  0.000000\n",
       "2     x3  0.000337\n",
       "3     x4  0.005211\n",
       "4     x5  0.000000\n",
       "5     x6  0.000000\n",
       "6     x7  0.001330\n",
       "7     x8  0.014919\n",
       "8     x9  0.000000\n",
       "9    x10  0.000000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_df"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
