{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.8\n",
    "* change hard code column index to column names (change matrix to dataframe, change index to column name) [DONE]\n",
    "* use sklearn to do the splitting [DONE]\n",
    "* calculate and print out proportion (1. # of data in a node / total #, 2. proportion of trt and ctl) [DONE]\n",
    "* change the output into a sklearn tree structure [CAN'T FIND]\n",
    "* Incorporate the global variable [DONE]\n",
    "* implement prediction function [DONE]\n",
    "\n",
    "7.9:\n",
    "* plot the tree somehow [in progress]\n",
    "    * rpart.plot(x) uses a similar way to store the tree structure. Maybe find the rpart code and translate into python\n",
    "* implement random forest [DONE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to build tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def causal_train_test_split(data, predictors, response, treatment, test_size = 0.25, estimation_size = 0.33):\n",
    "    \n",
    "    #define PROP as a global variable to use later\n",
    "    global PROP\n",
    "    PROP = 1 - estimation_size\n",
    "    \n",
    "    #split the whole set into training set and test set\n",
    "    train_set, test_set = train_test_split(data, test_size=test_size)\n",
    "    #split the training set into training sample and estimation sample\n",
    "    training_sample, estimation_sample = train_test_split(train_set, test_size=estimation_size)\n",
    "    \n",
    "    #randomly assign labels to training sample and estimation sample\n",
    "    training_sample.insert(loc = 0, column = 'TRAIN_ESTIMATION_IND', value = np.ones(len(training_sample)))\n",
    "    estimation_sample.insert(loc = 0, column = 'TRAIN_ESTIMATION_IND', value = np.zeros(len(estimation_sample)))\n",
    "    new_train_set = pd.concat([training_sample, estimation_sample])\n",
    "    \n",
    "    #take the subset of the original dataset with necessary columns\n",
    "    new_train_set = new_train_set[['TRAIN_ESTIMATION_IND'] + predictors + treatment + response]\n",
    "    test_set = test_set[predictors + treatment + response]\n",
    "    \n",
    "    return new_train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_split(index, value, dataset):\n",
    "    \"\"\" \n",
    "    A function seperate a dataset into two numpy matrices \n",
    "    given the index of an attribute and a split value for that attribute\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "        index(int): the index of the columns of the dataset\n",
    "        value(float): the value to be compared with\n",
    "        dataset(numpy array): the dataset to split\n",
    "    \n",
    "    Output:\n",
    "    ------:\n",
    "        left(numpy array): the dataset that is split(left half)\n",
    "        right(numpy array): the dataset that is split(right half)\n",
    "    \n",
    "    \"\"\"\n",
    "    #get the number of columns of the matrix\n",
    "    dim = dataset.shape[1]\n",
    "    \n",
    "    left, right = np.empty(shape=[0, dim]), np.empty(shape=[0, dim])\n",
    "    \n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left = np.append(left, [row], axis = 0)\n",
    "        else:\n",
    "            right = np.append(right, [row], axis = 0)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_emse(train, est, row, index):\n",
    "    \n",
    "    # check the cardinality of the training and estimation samples, if size < *threshold*\n",
    "    # then can not do the calculation\n",
    "    train_size = len(train)\n",
    "    est_size = len(est)\n",
    "    \n",
    "    # split both training sample and estimation sample using the same rule\n",
    "    left_train, right_train = data_split(index, row[index], train)\n",
    "    left_est, right_est = data_split(index, row[index], est)\n",
    "    \n",
    "\n",
    "    ### Calculate the estimated treatment effect\n",
    "            \n",
    "    # get the cardinality of training sample for both leaves\n",
    "    left_train_size = len(left_train)\n",
    "    right_train_size = len(right_train)\n",
    "    \n",
    "    # calculate the treatment effect for both leaves, \n",
    "    left_est_response_trt = get_response(left_est, 'treatment') \n",
    "    left_est_response_ctl = get_response(left_est, 'control') \n",
    "    right_est_response_trt = get_response(right_est, 'treatment') \n",
    "    right_est_response_ctl = get_response(right_est, 'control') \n",
    "    #check cardinality of each leaf, make sure each leaf has at least *threshold* (chould be changed by user)\n",
    "    # treatment and n control to do the calculation\n",
    "    \n",
    "    left_trt_effect = left_est_response_trt.mean() - left_est_response_ctl.mean()\n",
    "    right_trt_effect = right_est_response_trt.mean() - right_est_response_ctl.mean()\n",
    "    \n",
    "    # then calculated the estimated squared treatment effect\n",
    "    e_trt_effect = (left_train_size * (left_trt_effect ** 2) + right_train_size * (right_trt_effect ** 2))/(train_size)\n",
    "    \n",
    "            \n",
    "    ### Calculate the estimated variance\n",
    "    left_var = np.var(left_est_response_trt) / PROP + np.var(left_est_response_ctl) / (1 - PROP)\n",
    "    right_var = np.var(right_est_response_trt) / PROP + np.var(right_est_response_ctl) / (1 - PROP)\n",
    "    e_var = (1 / train_size + 1 / est_size) * (left_var + right_var)\n",
    "    \n",
    "    \n",
    "    ### Calculate EMSE\n",
    "    emse = e_trt_effect - e_var    \n",
    "    \n",
    "    return emse\n",
    "    \n",
    "def get_split_emse(dataset):\n",
    " \n",
    "    train = dataset[dataset[:,0] == 1]\n",
    "    est = dataset[dataset[:,0] == 0]\n",
    "\n",
    "\n",
    "    # initialize values to return\n",
    "    b_index, b_value, b_score, b_groups = float('inf'), float('inf'), float('-inf'), None\n",
    "    \n",
    "    for index in range(1, train.shape[1] - 2):\n",
    "        for row in train:\n",
    "            groups = data_split(index, row[index], dataset)\n",
    "            emse = get_emse(train, est, row, index)\n",
    "            # if mse score gets improved (reduced actually), update the information\n",
    "            if emse > b_score:# and emse is not np.nan:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], emse, groups   \n",
    "                \n",
    "    ret_dict =  {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_response(dataset, trt):\n",
    "    if trt == 'treatment':\n",
    "        return dataset[dataset[:,-2] == 1][:,-1]\n",
    "    elif trt == 'control':\n",
    "        return dataset[dataset[:,-2] == 0][:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the split based on criterion\n",
    "def get_split(dataset, criterion):\n",
    "    \"\"\" \n",
    "    A function to split the data based on splitting criterion specified by user\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "        dataset(np array): a dataset in the form of a numpy matrix\n",
    "        criterion(str): a str to indicate the criterion specified by user\n",
    "    \n",
    "    Output:\n",
    "    ------:\n",
    "        the same output as functions get_split_xxx\n",
    "    \n",
    "    \"\"\"    \n",
    "    if criterion == 'mse':\n",
    "        return get_split_mse(dataset)\n",
    "    if criterion == 'causal':\n",
    "        return get_split_emse(dataset)    \n",
    "    elif criterion == 'gini':\n",
    "        return get_split_gini(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a terminal node value\n",
    "def to_terminal_gini(group):\n",
    "    response = group[:,-1]\n",
    "    return stats.mode(response)[0][0] # this could be optimized\n",
    "\n",
    "def to_terminal_mse(group):\n",
    "    response = group[:,-1]\n",
    "    return np.mean(response)\n",
    "\n",
    "def to_terminal_emse(group):\n",
    "    est_trt = get_response(group, 'treatment')\n",
    "    est_ctl = get_response(group, 'control')\n",
    "    \n",
    "    causal_effect = np.mean(est_trt) - np.mean(est_ctl)\n",
    "    proportion_of_data = (len(est_trt) + len(est_ctl)) / TOTAL_DATA_COUNT\n",
    "    \n",
    "    return round(causal_effect, 3), round(proportion_of_data * 100, 1)\n",
    "    \n",
    "    \n",
    "def to_terminal(group, criterion):\n",
    "    if criterion == 'gini':\n",
    "        return to_terminal_gini(group)\n",
    "    elif criterion == 'mse':\n",
    "        return to_terminal_mse(group)\n",
    "    elif criterion == 'causal':\n",
    "        return to_terminal_emse(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth, criterion):\n",
    "    \n",
    "    left, right = node['groups']\n",
    "    \n",
    "    left_train = left[left[:,0] == 1]\n",
    "    left_est = left[left[:,0] == 0]\n",
    "    right_train = right[right[:,0] == 1]\n",
    "    right_est = right[right[:,0] == 0]    \n",
    "    \n",
    "    left_train_response_trt = get_response(left_train, 'treatment')\n",
    "    left_train_response_ctl = get_response(left_train, 'control')\n",
    "    right_train_response_trt = get_response(right_train, 'treatment')\n",
    "    right_train_response_ctl = get_response(right_train, 'control')  \n",
    "    \n",
    "    left_est_response_trt = get_response(left_est, 'treatment')\n",
    "    left_est_response_ctl = get_response(left_est, 'control')\n",
    "    right_est_response_trt = get_response(right_est, 'treatment')\n",
    "    right_est_response_ctl = get_response(right_est, 'control')\n",
    "    \n",
    "    del(node['groups'])\n",
    "    \n",
    "    if len(left) == 0 or len(right) == 0:\n",
    "        node['left'] = node['right'] = to_terminal(np.append(left, right, axis = 0), criterion)\n",
    "        return\n",
    "    \n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left, criterion), to_terminal(right, criterion)\n",
    "        return\n",
    "    \n",
    "    # process left child\n",
    "    if (len(left) <= min_size or len(left_est_response_trt) <= min_size or len(left_est_response_ctl) <= min_size or\n",
    "        len(right_est_response_trt) <= min_size or len(right_est_response_ctl) <= min_size or \n",
    "        len(left_train_response_trt) <= min_size or len(left_train_response_ctl) <= min_size or \n",
    "        len(right_train_response_trt) <= min_size or len(right_train_response_ctl) <= min_size):\n",
    "        node['left'] = to_terminal(left, criterion)\n",
    "    else:\n",
    "        node['left'] = get_split(left, criterion)\n",
    "        if node['left']['groups'] is None:\n",
    "            node['left'] = to_terminal(left, criterion)\n",
    "        else:\n",
    "            split(node['left'], max_depth, min_size, depth+1, criterion)\n",
    "        \n",
    "    # process right child\n",
    "    if (len(right) <= min_size or len(left_est_response_trt) <= min_size or len(left_est_response_ctl) <= min_size or\n",
    "        len(right_est_response_trt) <= min_size or len(right_est_response_ctl) <= min_size or \n",
    "        len(left_train_response_trt) <= min_size or len(left_train_response_ctl) <= min_size or \n",
    "        len(right_train_response_trt) <= min_size or len(right_train_response_ctl) <= min_size):\n",
    "        node['right'] = to_terminal(right, criterion)\n",
    "    else:\n",
    "        node['right'] = get_split(right, criterion)\n",
    "        if node['right']['groups'] is None:\n",
    "            node['right'] = to_terminal(right, criterion)\n",
    "        else:\n",
    "            split(node['right'], max_depth, min_size, depth+1, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a decision tree\n",
    "def causalTree(train, max_depth, min_size, criterion):\n",
    "    \n",
    "    global TOTAL_DATA_COUNT, COLUMN_NAMES\n",
    "    TOTAL_DATA_COUNT = len(train)\n",
    "    COLUMN_NAMES = train.columns[1:-2]\n",
    "    \n",
    "    train = np.array(train)\n",
    "    \n",
    "    if criterion == 'mse' or criterion == 'gini':\n",
    "        root = get_split(train, criterion)\n",
    "        #print(root)\n",
    "        split(root, max_depth, min_size, 1, criterion)\n",
    "        \n",
    "    elif criterion == 'causal':\n",
    "        root = get_split(train, criterion)\n",
    "        split(root, max_depth, min_size, 1, criterion)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to use causal tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print out the tree structure\n",
    "def print_tree(node, depth=0):\n",
    "    if isinstance(node, dict):\n",
    "        print('%s[%s < %.3f]' % ((depth * ' ', (COLUMN_NAMES[node['index'] - 1]), node['value'])))\n",
    "        print_tree(node['left'], depth + 1)\n",
    "        print_tree(node['right'], depth + 1)\n",
    "    else:\n",
    "        print('%s[%s, %s%%]' % ((depth * ' ', node[0], node[1])))\n",
    "        \n",
    "#count the number of leaves in a tree\n",
    "def count_leaves(node):\n",
    "    if isinstance(node, tuple) == True:\n",
    "        return 1\n",
    "    else:\n",
    "        return count_leaves(node['left']) + count_leaves(node['right'])\n",
    "        \n",
    "        \n",
    "#causal effect prediction\n",
    "def causalPredict_helper(node,row):\n",
    "    if row[node['index'] - 1] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return causalPredict_helper(node['left'], row)\n",
    "        else:\n",
    "            return node['left'][0]\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return causalPredict_helper(node['right'], row)\n",
    "        else:\n",
    "            return node['right'][0]    \n",
    "            \n",
    "def causalPredict(test, tree):\n",
    "    #get the information of the trainning set and initialize an empty return dataframe\n",
    "    column_names = list(test.columns) + ['pred_causal_effect']\n",
    "    test_matrix = np.array(test)\n",
    "    ret_matrix = np.empty([0, test_matrix.shape[1] + 1])\n",
    "    \n",
    "    #predict for each row\n",
    "    for row in test_matrix:\n",
    "        row = np.insert(row, len(row), causalPredict_helper(tree, row))\n",
    "        ret_matrix = np.append(ret_matrix, [row], axis = 0) \n",
    "    \n",
    "    #return a new dataframe with the predicted value at the end of each row\n",
    "    ret_df = pd.DataFrame(ret_matrix, columns = column_names)\n",
    "\n",
    "    return ret_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### causal tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read in data\n",
    "df = pd.read_csv('test_df.csv')\n",
    "#df = df.iloc[1:500,:]\n",
    "\n",
    "#get the column names of predictors\n",
    "p_str = ['x1', 'x2']\n",
    "#get the column name of response\n",
    "r_str = ['y']\n",
    "#get the column name of treatment\n",
    "t_str = ['trt']\n",
    "\n",
    "#set a random seed for replication \n",
    "np.random.seed(123)\n",
    "\n",
    "#split the data\n",
    "train_set, test_set = causal_train_test_split(df, predictors = p_str, response = r_str, treatment = t_str)\n",
    "#build a causalTree\n",
    "tree_test  = causalTree(train_set, max_depth = 3, min_size = 10, criterion = 'causal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[x1 < 6.642]\n",
      " [2.873866323777404, 47.1%]\n",
      " [x1 < 7.999]\n",
      "  [-3.189864082840239, 29.2%]\n",
      "  [-0.980188170731707, 23.7%]\n"
     ]
    }
   ],
   "source": [
    "print_tree(tree_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>trt</th>\n",
       "      <th>y</th>\n",
       "      <th>pred_causal_effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.961043</td>\n",
       "      <td>2.619950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.12341</td>\n",
       "      <td>2.873866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.999209</td>\n",
       "      <td>2.209014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.01134</td>\n",
       "      <td>2.873866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.497546</td>\n",
       "      <td>3.162954</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.56433</td>\n",
       "      <td>-3.189864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.124939</td>\n",
       "      <td>3.234551</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.84662</td>\n",
       "      <td>-0.980188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.678320</td>\n",
       "      <td>2.812814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.92357</td>\n",
       "      <td>2.873866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x1        x2  trt        y  pred_causal_effect\n",
       "0   3.961043  2.619950  0.0  2.12341            2.873866\n",
       "1   2.999209  2.209014  0.0  1.01134            2.873866\n",
       "2   7.497546  3.162954  1.0  1.56433           -3.189864\n",
       "3  10.124939  3.234551  1.0  0.84662           -0.980188\n",
       "4   3.678320  2.812814  0.0  6.92357            2.873866"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = causalPredict(test_set, tree_test)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simulation study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fake_response(df):\n",
    "    \n",
    "    #design 1\n",
    "    n_x_1 = 1/2*df['x1'] + df['x2']\n",
    "    k_x_1 = 1/2*df['x1']\n",
    "    \n",
    "    df['design1_y'] = n_x_1 + 1/2*(2*df['treatment'] - 1) * k_x_1 + df['error']\n",
    "    \n",
    "    #design 2\n",
    "    n_x_2 = 1/2 * (df['x1'] + df['x2']) + df['x3'] + df['x4'] + df['x5'] + df['x6']\n",
    "    \n",
    "    x1_pos = df['x1'].apply(lambda x: x if x > 0 else 0)\n",
    "    x2_pos = df['x2'].apply(lambda x: x if x > 0 else 0)\n",
    "    k_x_2 = x1_pos + x2_pos\n",
    "    \n",
    "    df['design2_y'] = n_x_2 + 1/2*(2*df['treatment'] - 1) * k_x_2 + df['error'] \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a fake data\n",
    "fake_data = pd.DataFrame()\n",
    "\n",
    "np.random.seed(123)\n",
    "fake_data['x1'] = np.random.normal(0, 0.12, 1000)\n",
    "fake_data['x2'] = np.random.normal(0.1, 0.2, 1000)\n",
    "fake_data['x3'] = np.random.gamma(1, 1.5, 1000)\n",
    "fake_data['x4'] = np.random.beta(1, 1, 1000)\n",
    "fake_data['x5'] = np.random.logistic(2, 1, 1000)\n",
    "fake_data['x6'] = np.random.gamma(2, 1, 1000)\n",
    "fake_data['x7'] = np.random.beta(1, 1.75, 1000)\n",
    "fake_data['x8'] = np.random.gamma(0.5, 0.5, 1000)\n",
    "fake_data['x9'] = np.random.beta(0.5, 0.5, 1000)\n",
    "fake_data['x10'] = np.random.gamma(1.5, 1.5, 1000)\n",
    "fake_data['treatment'] = np.concatenate((np.zeros(500),np.ones(500)), axis = 0)\n",
    "fake_data['error'] = np.random.normal(0, 0.01, 1000)\n",
    "\n",
    "fake_data = fake_response(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fake_data.to_csv('fake_data_large.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>treatment</th>\n",
       "      <th>error</th>\n",
       "      <th>design1_y</th>\n",
       "      <th>design2_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.130276</td>\n",
       "      <td>-0.049765</td>\n",
       "      <td>0.794089</td>\n",
       "      <td>0.157970</td>\n",
       "      <td>0.830434</td>\n",
       "      <td>0.932308</td>\n",
       "      <td>0.222469</td>\n",
       "      <td>0.201090</td>\n",
       "      <td>0.548776</td>\n",
       "      <td>0.817653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.012080</td>\n",
       "      <td>-0.094414</td>\n",
       "      <td>2.612701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.119681</td>\n",
       "      <td>0.213519</td>\n",
       "      <td>0.689766</td>\n",
       "      <td>0.933238</td>\n",
       "      <td>1.699821</td>\n",
       "      <td>3.747315</td>\n",
       "      <td>0.440415</td>\n",
       "      <td>0.209729</td>\n",
       "      <td>0.963722</td>\n",
       "      <td>7.540478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.013532</td>\n",
       "      <td>0.229907</td>\n",
       "      <td>7.056607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033957</td>\n",
       "      <td>0.243630</td>\n",
       "      <td>1.756790</td>\n",
       "      <td>0.509386</td>\n",
       "      <td>1.989609</td>\n",
       "      <td>2.818948</td>\n",
       "      <td>0.608559</td>\n",
       "      <td>0.247184</td>\n",
       "      <td>0.442523</td>\n",
       "      <td>4.141402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001522</td>\n",
       "      <td>0.250598</td>\n",
       "      <td>7.073211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.180755</td>\n",
       "      <td>-0.099876</td>\n",
       "      <td>2.031662</td>\n",
       "      <td>0.820777</td>\n",
       "      <td>1.183160</td>\n",
       "      <td>0.314951</td>\n",
       "      <td>0.439215</td>\n",
       "      <td>0.034655</td>\n",
       "      <td>0.386692</td>\n",
       "      <td>0.147139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.009023</td>\n",
       "      <td>-0.154088</td>\n",
       "      <td>4.201213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.069432</td>\n",
       "      <td>0.194980</td>\n",
       "      <td>0.829197</td>\n",
       "      <td>0.989342</td>\n",
       "      <td>0.037687</td>\n",
       "      <td>2.960247</td>\n",
       "      <td>0.550352</td>\n",
       "      <td>0.381596</td>\n",
       "      <td>0.994200</td>\n",
       "      <td>5.076362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000364</td>\n",
       "      <td>0.177258</td>\n",
       "      <td>4.781393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2        x3        x4        x5        x6        x7  \\\n",
       "0 -0.130276 -0.049765  0.794089  0.157970  0.830434  0.932308  0.222469   \n",
       "1  0.119681  0.213519  0.689766  0.933238  1.699821  3.747315  0.440415   \n",
       "2  0.033957  0.243630  1.756790  0.509386  1.989609  2.818948  0.608559   \n",
       "3 -0.180755 -0.099876  2.031662  0.820777  1.183160  0.314951  0.439215   \n",
       "4 -0.069432  0.194980  0.829197  0.989342  0.037687  2.960247  0.550352   \n",
       "\n",
       "         x8        x9       x10  treatment     error  design1_y  design2_y  \n",
       "0  0.201090  0.548776  0.817653        0.0 -0.012080  -0.094414   2.612701  \n",
       "1  0.209729  0.963722  7.540478        0.0 -0.013532   0.229907   7.056607  \n",
       "2  0.247184  0.442523  4.141402        0.0 -0.001522   0.250598   7.073211  \n",
       "3  0.034655  0.386692  0.147139        0.0 -0.009023  -0.154088   4.201213  \n",
       "4  0.381596  0.994200  5.076362        0.0 -0.000364   0.177258   4.781393  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### design 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 0\n",
      "[x1 < 0.040]\n",
      " [x1 < 0.036]\n",
      "  [-0.078, 64.1%]\n",
      "  [-0.089, 1.5%]\n",
      " [x2 < 0.143]\n",
      "  [0.08, 20.1%]\n",
      "  [0.073, 14.3%]\n",
      "Tree 1\n",
      "[x1 < -0.039]\n",
      " [x1 < -0.151]\n",
      "  [-0.206, 11.6%]\n",
      "  [-0.083, 26.9%]\n",
      " [x1 < -0.034]\n",
      "  [0.34, 1.0%]\n",
      "  [0.03, 60.5%]\n",
      "Tree 2\n",
      "[x1 < -0.047]\n",
      " [x1 < -0.061]\n",
      "  [-0.139, 31.1%]\n",
      "  [-0.082, 4.2%]\n",
      " [x1 < 0.218]\n",
      "  [0.032, 61.9%]\n",
      "  [0.061, 2.8%]\n",
      "Tree 3\n",
      "[x1 < -0.054]\n",
      " [x1 < -0.066]\n",
      "  [-0.14, 30.0%]\n",
      "  [-0.168, 2.7%]\n",
      " [x2 < 0.061]\n",
      "  [0.025, 27.3%]\n",
      "  [0.039, 40.0%]\n",
      "Tree 4\n",
      "[x1 < 0.034]\n",
      " [x1 < -0.151]\n",
      "  [-0.21, 11.5%]\n",
      "  [-0.051, 52.2%]\n",
      " [x2 < 0.007]\n",
      "  [0.079, 12.5%]\n",
      "  [0.073, 23.8%]\n"
     ]
    }
   ],
   "source": [
    "predictors = ['x1','x2']\n",
    "response_1 = ['design1_y']\n",
    "treatment = ['treatment']\n",
    "for i in range(5):\n",
    "    print('Tree ' + str(i))\n",
    "    train_set, test_set = causal_train_test_split(fake_data, predictors, response_1, treatment, test_size = 0, estimation_size = 0.5)\n",
    "    tree_1  = causalTree(train_set, max_depth = 2, min_size = 25, criterion = 'causal')\n",
    "    print_tree(tree_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### design 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predictors = ['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10']\n",
    "# response_2 = ['design2_y']\n",
    "# treatment = ['treatment']\n",
    "# for i in range(5):\n",
    "#     print('Tree ' + str(i))\n",
    "#     train_set, test_set = causal_train_test_split(fake_data, predictors, response_2, treatment, test_size = 0, estimation_size = 0.5)\n",
    "#     tree_2  = causalTree(train_set, max_depth = 5, min_size = 10, criterion = 'causal')\n",
    "#     print_tree(tree_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
