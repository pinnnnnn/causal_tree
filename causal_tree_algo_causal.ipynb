{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.8\n",
    "* change hard code column index to column names (change matrix to dataframe, change index to column name) [DONE]\n",
    "* use sklearn to do the splitting [DONE]\n",
    "* calculate and print out proportion (1. # of data in a node / total #, 2. proportion of trt and ctl) [DONE]\n",
    "* change the output into a sklearn tree structure [CAN'T FIND]\n",
    "* Incorporate the global variable [DONE]\n",
    "* implement prediction function [DONE]\n",
    "\n",
    "7.9:\n",
    "* plot the tree somehow [in progress]\n",
    "    * rpart.plot(x) uses a similar way to store the tree structure. Maybe find the rpart code and translate into python\n",
    "* implement random forest [DONE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to build tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def causal_train_test_split(data, predictors, response, treatment, test_size = 0.25, estimation_size = 0.33):\n",
    "    \n",
    "    #define PROP as a global variable to use later\n",
    "    global PROP\n",
    "    PROP = 1 - estimation_size\n",
    "    \n",
    "    #split the whole set into training set and test set\n",
    "    train_set, test_set = train_test_split(data, test_size=test_size)\n",
    "    #split the training set into training sample and estimation sample\n",
    "    training_sample, estimation_sample = train_test_split(train_set, test_size=estimation_size)\n",
    "    \n",
    "    #randomly assign labels to training sample and estimation sample\n",
    "    training_sample.insert(loc = 0, column = 'TRAIN_ESTIMATION_IND', value = np.ones(len(training_sample)))\n",
    "    estimation_sample.insert(loc = 0, column = 'TRAIN_ESTIMATION_IND', value = np.zeros(len(estimation_sample)))\n",
    "    new_train_set = pd.concat([training_sample, estimation_sample])\n",
    "    \n",
    "    #take the subset of the original dataset with necessary columns\n",
    "    new_train_set = new_train_set[['TRAIN_ESTIMATION_IND'] + predictors + treatment + response]\n",
    "    test_set = test_set[predictors + treatment + response]\n",
    "    \n",
    "    return new_train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_split(index, value, dataset):\n",
    "    \"\"\" \n",
    "    A function seperate a dataset into two numpy matrices \n",
    "    given the index of an attribute and a split value for that attribute\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "        index(int): the index of the columns of the dataset\n",
    "        value(float): the value to be compared with\n",
    "        dataset(numpy array): the dataset to split\n",
    "    \n",
    "    Output:\n",
    "    ------:\n",
    "        left(numpy array): the dataset that is split(left half)\n",
    "        right(numpy array): the dataset that is split(right half)\n",
    "    \n",
    "    \"\"\"\n",
    "    #get the number of columns of the matrix\n",
    "    dim = dataset.shape[1]\n",
    "    \n",
    "    left, right = np.empty(shape=[0, dim]), np.empty(shape=[0, dim])\n",
    "    \n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left = np.append(left, [row], axis = 0)\n",
    "        else:\n",
    "            right = np.append(right, [row], axis = 0)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_emse(train, est, row, index):\n",
    "    \n",
    "    # check the cardinality of the training and estimation samples, if size < *threshold*\n",
    "    # then can not do the calculation\n",
    "    train_size = len(train)\n",
    "    est_size = len(est)\n",
    "    \n",
    "    # split both training sample and estimation sample using the same rule\n",
    "    left_train, right_train = data_split(index, row[index], train)\n",
    "    left_est, right_est = data_split(index, row[index], est)\n",
    "    \n",
    "\n",
    "    ### Calculate the estimated treatment effect\n",
    "            \n",
    "    # get the cardinality of training sample for both leaves\n",
    "    left_train_size = len(left_train)\n",
    "    right_train_size = len(right_train)\n",
    "    \n",
    "    # calculate the treatment effect for both leaves, \n",
    "    left_est_response_trt = get_response(left_est, 'treatment') \n",
    "    left_est_response_ctl = get_response(left_est, 'control') \n",
    "    right_est_response_trt = get_response(right_est, 'treatment') \n",
    "    right_est_response_ctl = get_response(right_est, 'control') \n",
    "    #check cardinality of each leaf, make sure each leaf has at least *threshold* (chould be changed by user)\n",
    "    # treatment and n control to do the calculation\n",
    "    \n",
    "    left_trt_effect = left_est_response_trt.mean() - left_est_response_ctl.mean()\n",
    "    right_trt_effect = right_est_response_trt.mean() - right_est_response_ctl.mean()\n",
    "    \n",
    "    # then calculated the estimated squared treatment effect\n",
    "    e_trt_effect = (left_train_size * (left_trt_effect ** 2) + right_train_size * (right_trt_effect ** 2))/(train_size)\n",
    "    \n",
    "            \n",
    "    ### Calculate the estimated variance\n",
    "    left_var = np.var(left_est_response_trt) / PROP + np.var(left_est_response_ctl) / (1 - PROP)\n",
    "    right_var = np.var(right_est_response_trt) / PROP + np.var(right_est_response_ctl) / (1 - PROP)\n",
    "    e_var = (1 / train_size + 1 / est_size) * (left_var + right_var)\n",
    "    \n",
    "    \n",
    "    ### Calculate EMSE\n",
    "    emse = e_trt_effect - e_var    \n",
    "    \n",
    "    return emse\n",
    "    \n",
    "def get_split_emse(dataset):\n",
    " \n",
    "    train = dataset[dataset[:,0] == 1]\n",
    "    est = dataset[dataset[:,0] == 0]\n",
    "\n",
    "\n",
    "    # initialize values to return\n",
    "    b_index, b_value, b_score, b_groups = float('inf'), float('inf'), float('-inf'), None\n",
    "    \n",
    "    for index in range(1, train.shape[1] - 2):\n",
    "        for row in train:\n",
    "            groups = data_split(index, row[index], dataset)\n",
    "            emse = get_emse(train, est, row, index)\n",
    "            # if mse score gets improved (reduced actually), update the information\n",
    "            if emse > b_score:# and emse is not np.nan:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], emse, groups   \n",
    "                \n",
    "    ret_dict =  {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sum_F_stats(dataset, left, right):\n",
    "    \n",
    "    #get response as lists \n",
    "    all_response = train[:,-1]\n",
    "    left_response = left[:,-1]\n",
    "    right_response = right[:,-1]\n",
    "    left_trt_response = get_response(left, 'treatment')\n",
    "    right_trt_response = get_response(right, 'treatment')\n",
    "    left_ctl_response = get_response(left, 'control')\n",
    "    right_ctl_response = get_response(right, 'control')\n",
    "    \n",
    "    #get variance of all of the response \n",
    "    N = len(train) #500\n",
    "    S_nosplit = np.var(all_response) \n",
    "    \n",
    "    n_left = len(left)\n",
    "    n_right = len(right)\n",
    "    S_split = ((n_left - 1) * np.var(left_response) + (n_right - 1) * np.var(right_response))/(n_left + n_right - 1) \n",
    "    #get the T statistics squared for left and right node\n",
    "    T_0_sqr = ((left_ctl_response.mean()-right_ctl_response.mean())**2)/(S_split/len(left_ctl_response)+S_split/len(right_ctl_response))\n",
    "    T_1_sqr = ((left_trt_response.mean()-right_trt_response.mean())**2)/(S_split/len(left_trt_response)+S_split/len(right_trt_response))\n",
    "    T_sum = T_0_sqr + T_1_sqr\n",
    "    \n",
    "    #Calculate F statistics\n",
    "    F = S_nosplit* ((2*T_sum)/(1+2*T_sum/N))\n",
    "    \n",
    "    #Calculate p value\n",
    "    p_value = scipy.stats.f.cdf(F, df1, df2)\n",
    "    \n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_response(dataset, trt):\n",
    "    if trt == 'treatment':\n",
    "        return dataset[dataset[:,-2] == 1][:,-1]\n",
    "    elif trt == 'control':\n",
    "        return dataset[dataset[:,-2] == 0][:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the split based on criterion\n",
    "def get_split(dataset, criterion):\n",
    "    \"\"\" \n",
    "    A function to split the data based on splitting criterion specified by user\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "        dataset(np array): a dataset in the form of a numpy matrix\n",
    "        criterion(str): a str to indicate the criterion specified by user\n",
    "    \n",
    "    Output:\n",
    "    ------:\n",
    "        the same output as functions get_split_xxx\n",
    "    \n",
    "    \"\"\"    \n",
    "    if criterion == 'mse':\n",
    "        return get_split_mse(dataset)\n",
    "    if criterion == 'causal':\n",
    "        return get_split_emse(dataset)    \n",
    "    elif criterion == 'gini':\n",
    "        return get_split_gini(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a terminal node value\n",
    "def to_terminal_gini(group):\n",
    "    response = group[:,-1]\n",
    "    return stats.mode(response)[0][0] # this could be optimized\n",
    "\n",
    "def to_terminal_mse(group):\n",
    "    response = group[:,-1]\n",
    "    return np.mean(response)\n",
    "\n",
    "def to_terminal_emse(group):\n",
    "    est_trt = get_response(group, 'treatment')\n",
    "    est_ctl = get_response(group, 'control')\n",
    "    \n",
    "    causal_effect = np.mean(est_trt) - np.mean(est_ctl)\n",
    "    proportion_of_data = (len(est_trt) + len(est_ctl)) / TOTAL_DATA_COUNT\n",
    "    \n",
    "    return round(causal_effect, 3), round(proportion_of_data * 100, 1)\n",
    "    \n",
    "    \n",
    "def to_terminal(group, criterion):\n",
    "    if criterion == 'gini':\n",
    "        return to_terminal_gini(group)\n",
    "    elif criterion == 'mse':\n",
    "        return to_terminal_mse(group)\n",
    "    elif criterion == 'causal':\n",
    "        return to_terminal_emse(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth, criterion):\n",
    "    \n",
    "    left, right = node['groups']\n",
    "    \n",
    "    left_train = left[left[:,0] == 1]\n",
    "    left_est = left[left[:,0] == 0]\n",
    "    right_train = right[right[:,0] == 1]\n",
    "    right_est = right[right[:,0] == 0]    \n",
    "    \n",
    "    left_train_response_trt = get_response(left_train, 'treatment')\n",
    "    left_train_response_ctl = get_response(left_train, 'control')\n",
    "    right_train_response_trt = get_response(right_train, 'treatment')\n",
    "    right_train_response_ctl = get_response(right_train, 'control')  \n",
    "    \n",
    "    left_est_response_trt = get_response(left_est, 'treatment')\n",
    "    left_est_response_ctl = get_response(left_est, 'control')\n",
    "    right_est_response_trt = get_response(right_est, 'treatment')\n",
    "    right_est_response_ctl = get_response(right_est, 'control')\n",
    "    \n",
    "    del(node['groups'])\n",
    "    \n",
    "    if len(left) == 0 or len(right) == 0:\n",
    "        node['left'] = node['right'] = to_terminal(np.append(left, right, axis = 0), criterion)\n",
    "        return\n",
    "    \n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left, criterion), to_terminal(right, criterion)\n",
    "        return\n",
    "    \n",
    "    # process left child\n",
    "    if (len(left) <= min_size or len(left_est_response_trt) <= min_size or len(left_est_response_ctl) <= min_size or\n",
    "        len(right_est_response_trt) <= min_size or len(right_est_response_ctl) <= min_size or \n",
    "        len(left_train_response_trt) <= min_size or len(left_train_response_ctl) <= min_size or \n",
    "        len(right_train_response_trt) <= min_size or len(right_train_response_ctl) <= min_size):\n",
    "        node['left'] = to_terminal(left, criterion)\n",
    "    else:\n",
    "        node['left'] = get_split(left, criterion)\n",
    "        if node['left']['groups'] is None:\n",
    "            node['left'] = to_terminal(left, criterion)\n",
    "        else:\n",
    "            split(node['left'], max_depth, min_size, depth+1, criterion)\n",
    "        \n",
    "    # process right child\n",
    "    if (len(right) <= min_size or len(left_est_response_trt) <= min_size or len(left_est_response_ctl) <= min_size or\n",
    "        len(right_est_response_trt) <= min_size or len(right_est_response_ctl) <= min_size or \n",
    "        len(left_train_response_trt) <= min_size or len(left_train_response_ctl) <= min_size or \n",
    "        len(right_train_response_trt) <= min_size or len(right_train_response_ctl) <= min_size):\n",
    "        node['right'] = to_terminal(right, criterion)\n",
    "    else:\n",
    "        node['right'] = get_split(right, criterion)\n",
    "        if node['right']['groups'] is None:\n",
    "            node['right'] = to_terminal(right, criterion)\n",
    "        else:\n",
    "            split(node['right'], max_depth, min_size, depth+1, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a decision tree\n",
    "def causalTree(train, max_depth, min_size, criterion):\n",
    "    \n",
    "    global TOTAL_DATA_COUNT, COLUMN_NAMES\n",
    "    TOTAL_DATA_COUNT = len(train)\n",
    "    COLUMN_NAMES = train.columns[1:-2]\n",
    "    \n",
    "    train = np.array(train)\n",
    "    \n",
    "    if criterion == 'mse' or criterion == 'gini':\n",
    "        root = get_split(train, criterion)\n",
    "        #print(root)\n",
    "        split(root, max_depth, min_size, 1, criterion)\n",
    "        \n",
    "    elif criterion == 'causal':\n",
    "        root = get_split(train, criterion)\n",
    "        split(root, max_depth, min_size, 1, criterion)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to use causal tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print out the tree structure\n",
    "def print_tree(node, depth=0):\n",
    "    if isinstance(node, dict):\n",
    "        print('%s[%s < %.3f]' % ((depth * ' ', (COLUMN_NAMES[node['index'] - 1]), node['value'])))\n",
    "        print_tree(node['left'], depth + 1)\n",
    "        print_tree(node['right'], depth + 1)\n",
    "    else:\n",
    "        print('%s[%s, %s%%]' % ((depth * ' ', node[0], node[1])))\n",
    "        \n",
    "#count the number of leaves in a tree\n",
    "def count_leaves(node):\n",
    "    if isinstance(node, tuple) == True:\n",
    "        return 1\n",
    "    else:\n",
    "        return count_leaves(node['left']) + count_leaves(node['right'])\n",
    "        \n",
    "        \n",
    "#causal effect prediction\n",
    "def causalPredict_helper(node,row):\n",
    "    if row[node['index'] - 1] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return causalPredict_helper(node['left'], row)\n",
    "        else:\n",
    "            return node['left'][0]\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return causalPredict_helper(node['right'], row)\n",
    "        else:\n",
    "            return node['right'][0]    \n",
    "            \n",
    "def causalPredict(test, tree):\n",
    "    #get the information of the trainning set and initialize an empty return dataframe\n",
    "    column_names = list(test.columns) + ['pred_causal_effect']\n",
    "    test_matrix = np.array(test)\n",
    "    ret_matrix = np.empty([0, test_matrix.shape[1] + 1])\n",
    "    \n",
    "    #predict for each row\n",
    "    for row in test_matrix:\n",
    "        row = np.insert(row, len(row), causalPredict_helper(tree, row))\n",
    "        ret_matrix = np.append(ret_matrix, [row], axis = 0) \n",
    "    \n",
    "    #return a new dataframe with the predicted value at the end of each row\n",
    "    ret_df = pd.DataFrame(ret_matrix, columns = column_names)\n",
    "\n",
    "    return ret_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### causal tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read in data\n",
    "df = pd.read_csv('test_df.csv')\n",
    "#df = df.iloc[1:500,:]\n",
    "\n",
    "#get the column names of predictors\n",
    "p_str = ['x1', 'x2']\n",
    "#get the column name of response\n",
    "r_str = ['y']\n",
    "#get the column name of treatment\n",
    "t_str = ['trt']\n",
    "\n",
    "#set a random seed for replication \n",
    "np.random.seed(123)\n",
    "\n",
    "#split the data\n",
    "train_set, test_set = causal_train_test_split(df, predictors = p_str, response = r_str, treatment = t_str)\n",
    "#build a causalTree\n",
    "tree_test  = causalTree(train_set, max_depth = 3, min_size = 10, criterion = 'causal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[x1 < 6.642]\n",
      " [2.874, 47.1%]\n",
      " [x1 < 7.999]\n",
      "  [-3.19, 29.2%]\n",
      "  [-0.98, 23.7%]\n"
     ]
    }
   ],
   "source": [
    "print_tree(tree_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>trt</th>\n",
       "      <th>y</th>\n",
       "      <th>pred_causal_effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.961043</td>\n",
       "      <td>2.619950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.12341</td>\n",
       "      <td>2.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.999209</td>\n",
       "      <td>2.209014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.01134</td>\n",
       "      <td>2.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.497546</td>\n",
       "      <td>3.162954</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.56433</td>\n",
       "      <td>-3.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.124939</td>\n",
       "      <td>3.234551</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.84662</td>\n",
       "      <td>-0.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.678320</td>\n",
       "      <td>2.812814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.92357</td>\n",
       "      <td>2.874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x1        x2  trt        y  pred_causal_effect\n",
       "0   3.961043  2.619950  0.0  2.12341               2.874\n",
       "1   2.999209  2.209014  0.0  1.01134               2.874\n",
       "2   7.497546  3.162954  1.0  1.56433              -3.190\n",
       "3  10.124939  3.234551  1.0  0.84662              -0.980\n",
       "4   3.678320  2.812814  0.0  6.92357               2.874"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = causalPredict(test_set, tree_test)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simulation study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fake_response(df):\n",
    "    \n",
    "    #design 1\n",
    "    n_x_1 = 1/2*df['x1'] + df['x2']\n",
    "    k_x_1 = 1/2*df['x1']\n",
    "    \n",
    "    df['design1_y'] = n_x_1 + 1/2*(2*df['treatment'] - 1) * k_x_1 + df['error']\n",
    "    \n",
    "    #design 2\n",
    "    n_x_2 = 1/2 * (df['x1'] + df['x2']) + df['x3'] + df['x4'] + df['x5'] + df['x6']\n",
    "    x1_pos = df['x1'].apply(lambda x: x if x > 0 else 0)\n",
    "    x2_pos = df['x2'].apply(lambda x: x if x > 0 else 0)\n",
    "    k_x_2 = x1_pos + x2_pos\n",
    "    df['design2_y'] = n_x_2 + 1/2*(2*df['treatment'] - 1) * k_x_2 + df['error'] \n",
    "    \n",
    "    #design 3\n",
    "    n_x_3 = 1/2 * (df['x1'] + df['x2'] + df['x3'] + df['x4']) + df['x5'] + df['x6'] + df['x7'] + df['x8']\n",
    "    x3_pos = df['x3'].apply(lambda x: x if x > 0 else 0)\n",
    "    x4_pos = df['x4'].apply(lambda x: x if x > 0 else 0)  \n",
    "    k_x_3 = x1_pos + x2_pos + x3_pos + x4_pos\n",
    "    df['design3_y'] = n_x_3 + 1/2*(2*df['treatment'] - 1) * k_x_3 + df['error'] \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a fake data\n",
    "fake_data = pd.DataFrame()\n",
    "\n",
    "np.random.seed(123)\n",
    "fake_data['x1'] = np.random.normal(0, 0.12, 1000)\n",
    "fake_data['x2'] = np.random.normal(0.1, 0.2, 1000)\n",
    "fake_data['x3'] = np.random.gamma(1, 1.5, 1000)\n",
    "fake_data['x4'] = np.random.beta(1, 1, 1000)\n",
    "fake_data['x5'] = np.random.logistic(2, 1, 1000)\n",
    "fake_data['x6'] = np.random.gamma(2, 1, 1000)\n",
    "fake_data['x7'] = np.random.beta(1, 1.75, 1000)\n",
    "fake_data['x8'] = np.random.gamma(0.5, 0.5, 1000)\n",
    "fake_data['x9'] = np.random.beta(0.5, 0.5, 1000)\n",
    "fake_data['x10'] = np.random.gamma(1.5, 1.5, 1000)\n",
    "fake_data['x11'] = np.random.normal(0.5, 0.2, 1000)\n",
    "fake_data['x12'] = np.random.normal(-1, 0.8, 1000)\n",
    "fake_data['x13'] = np.random.gamma(1.5, 2, 1000)\n",
    "fake_data['x14'] = np.random.beta(0.78, 0.94, 1000)\n",
    "fake_data['x15'] = np.random.logistic(1, 0.26, 1000)\n",
    "fake_data['x16'] = np.random.gamma(3, 2, 1000)\n",
    "fake_data['x17'] = np.random.beta(2, 0.75, 1000)\n",
    "fake_data['x18'] = np.random.gamma(1.3, 1.3, 1000)\n",
    "fake_data['x19'] = np.random.beta(1.5, 1.5, 1000)\n",
    "fake_data['x20'] = np.random.gamma(0.45, 1.05, 1000)\n",
    "\n",
    "fake_data['treatment'] = np.concatenate((np.zeros(500),np.ones(500)), axis = 0)\n",
    "fake_data['error'] = np.random.normal(0, 0.01, 1000)\n",
    "\n",
    "fake_data = fake_response(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fake_data.to_csv('fake_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>x20</th>\n",
       "      <th>treatment</th>\n",
       "      <th>error</th>\n",
       "      <th>design1_y</th>\n",
       "      <th>design2_y</th>\n",
       "      <th>design3_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.130276</td>\n",
       "      <td>-0.049765</td>\n",
       "      <td>0.794089</td>\n",
       "      <td>0.157970</td>\n",
       "      <td>0.830434</td>\n",
       "      <td>0.932308</td>\n",
       "      <td>0.222469</td>\n",
       "      <td>0.201090</td>\n",
       "      <td>0.548776</td>\n",
       "      <td>0.817653</td>\n",
       "      <td>...</td>\n",
       "      <td>15.075963</td>\n",
       "      <td>0.989387</td>\n",
       "      <td>0.625170</td>\n",
       "      <td>0.590555</td>\n",
       "      <td>1.550304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.011629</td>\n",
       "      <td>-0.093964</td>\n",
       "      <td>2.613152</td>\n",
       "      <td>2.084651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.119681</td>\n",
       "      <td>0.213519</td>\n",
       "      <td>0.689766</td>\n",
       "      <td>0.933238</td>\n",
       "      <td>1.699821</td>\n",
       "      <td>3.747315</td>\n",
       "      <td>0.440415</td>\n",
       "      <td>0.209729</td>\n",
       "      <td>0.963722</td>\n",
       "      <td>7.540478</td>\n",
       "      <td>...</td>\n",
       "      <td>4.780359</td>\n",
       "      <td>0.704731</td>\n",
       "      <td>0.581564</td>\n",
       "      <td>0.570566</td>\n",
       "      <td>1.687521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034353</td>\n",
       "      <td>0.277792</td>\n",
       "      <td>7.104492</td>\n",
       "      <td>6.131633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033957</td>\n",
       "      <td>0.243630</td>\n",
       "      <td>1.756790</td>\n",
       "      <td>0.509386</td>\n",
       "      <td>1.989609</td>\n",
       "      <td>2.818948</td>\n",
       "      <td>0.608559</td>\n",
       "      <td>0.247184</td>\n",
       "      <td>0.442523</td>\n",
       "      <td>4.141402</td>\n",
       "      <td>...</td>\n",
       "      <td>10.945010</td>\n",
       "      <td>0.892283</td>\n",
       "      <td>2.333034</td>\n",
       "      <td>0.604520</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002844</td>\n",
       "      <td>0.249276</td>\n",
       "      <td>7.071889</td>\n",
       "      <td>5.661457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.180755</td>\n",
       "      <td>-0.099876</td>\n",
       "      <td>2.031662</td>\n",
       "      <td>0.820777</td>\n",
       "      <td>1.183160</td>\n",
       "      <td>0.314951</td>\n",
       "      <td>0.439215</td>\n",
       "      <td>0.034655</td>\n",
       "      <td>0.386692</td>\n",
       "      <td>0.147139</td>\n",
       "      <td>...</td>\n",
       "      <td>15.767905</td>\n",
       "      <td>0.832544</td>\n",
       "      <td>0.226002</td>\n",
       "      <td>0.667418</td>\n",
       "      <td>0.422643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010723</td>\n",
       "      <td>-0.134342</td>\n",
       "      <td>4.220958</td>\n",
       "      <td>1.842388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.069432</td>\n",
       "      <td>0.194980</td>\n",
       "      <td>0.829197</td>\n",
       "      <td>0.989342</td>\n",
       "      <td>0.037687</td>\n",
       "      <td>2.960247</td>\n",
       "      <td>0.550352</td>\n",
       "      <td>0.381596</td>\n",
       "      <td>0.994200</td>\n",
       "      <td>5.076362</td>\n",
       "      <td>...</td>\n",
       "      <td>5.410604</td>\n",
       "      <td>0.399005</td>\n",
       "      <td>1.409336</td>\n",
       "      <td>0.265912</td>\n",
       "      <td>0.086907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>0.182736</td>\n",
       "      <td>4.786871</td>\n",
       "      <td>3.900281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2        x3        x4        x5        x6        x7  \\\n",
       "0 -0.130276 -0.049765  0.794089  0.157970  0.830434  0.932308  0.222469   \n",
       "1  0.119681  0.213519  0.689766  0.933238  1.699821  3.747315  0.440415   \n",
       "2  0.033957  0.243630  1.756790  0.509386  1.989609  2.818948  0.608559   \n",
       "3 -0.180755 -0.099876  2.031662  0.820777  1.183160  0.314951  0.439215   \n",
       "4 -0.069432  0.194980  0.829197  0.989342  0.037687  2.960247  0.550352   \n",
       "\n",
       "         x8        x9       x10    ...            x16       x17       x18  \\\n",
       "0  0.201090  0.548776  0.817653    ...      15.075963  0.989387  0.625170   \n",
       "1  0.209729  0.963722  7.540478    ...       4.780359  0.704731  0.581564   \n",
       "2  0.247184  0.442523  4.141402    ...      10.945010  0.892283  2.333034   \n",
       "3  0.034655  0.386692  0.147139    ...      15.767905  0.832544  0.226002   \n",
       "4  0.381596  0.994200  5.076362    ...       5.410604  0.399005  1.409336   \n",
       "\n",
       "        x19       x20  treatment     error  design1_y  design2_y  design3_y  \n",
       "0  0.590555  1.550304        0.0 -0.011629  -0.093964   2.613152   2.084651  \n",
       "1  0.570566  1.687521        0.0  0.034353   0.277792   7.104492   6.131633  \n",
       "2  0.604520  0.000357        0.0 -0.002844   0.249276   7.071889   5.661457  \n",
       "3  0.667418  0.422643        0.0  0.010723  -0.134342   4.220958   1.842388  \n",
       "4  0.265912  0.086907        0.0  0.005115   0.182736   4.786871   3.900281  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### design 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 0\n",
      "[x1 < -0.125]\n",
      " [x1 < -0.258]\n",
      "  [-0.3, 1.7%]\n",
      "  [-0.163, 15.1%]\n",
      " [x1 < 0.046]\n",
      "  [-0.04, 50.8%]\n",
      "  [0.086, 32.4%]\n",
      "Tree 1\n",
      "[x1 < 0.040]\n",
      " [x1 < -0.188]\n",
      "  [-0.225, 6.7%]\n",
      "  [-0.06, 58.9%]\n",
      " [x1 < 0.222]\n",
      "  [0.087, 31.7%]\n",
      "  [0.059, 2.7%]\n",
      "Tree 2\n",
      "[x1 < -0.052]\n",
      " [x1 < -0.059]\n",
      "  [-0.138, 31.5%]\n",
      "  [-0.159, 1.7%]\n",
      " [x2 < 0.143]\n",
      "  [0.04, 37.9%]\n",
      "  [0.052, 28.9%]\n",
      "Tree 3\n",
      "[x1 < -0.028]\n",
      " [x1 < -0.227]\n",
      "  [-0.282, 3.1%]\n",
      "  [-0.098, 38.7%]\n",
      " [x2 < 0.044]\n",
      "  [0.038, 22.5%]\n",
      "  [0.051, 35.7%]\n",
      "Tree 4\n",
      "[x1 < -0.026]\n",
      " [x1 < -0.032]\n",
      "  [-0.11, 40.4%]\n",
      "  [-0.141, 2.3%]\n",
      " [x1 < 0.097]\n",
      "  [0.02, 36.6%]\n",
      "  [0.075, 20.7%]\n"
     ]
    }
   ],
   "source": [
    "predictors = ['x1','x2']\n",
    "response_1 = ['design1_y']\n",
    "treatment = ['treatment']\n",
    "for i in range(5):\n",
    "    print('Tree ' + str(i))\n",
    "    train_set, test_set = causal_train_test_split(fake_data, predictors, response_1, treatment, test_size = 0, estimation_size = 0.5)\n",
    "    tree_1  = causalTree(train_set, max_depth = 2, min_size = 25, criterion = 'causal')\n",
    "    print_tree(tree_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### design 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 0\n",
      "[x9 < 1.000]\n",
      " [-0.026, 99.4%]\n",
      " [2.481, 0.6%]\n",
      "Tree 1\n",
      "[x9 < 0.999]\n",
      " [-0.025, 98.7%]\n",
      " [1.126, 1.3%]\n",
      "Tree 2\n",
      "[x3 < 0.027]\n",
      " [-1.275, 2.2%]\n",
      " [0.016, 97.8%]\n",
      "Tree 3\n",
      "[x4 < 0.131]\n",
      " [x7 < 0.059]\n",
      "  [2.413, 1.2%]\n",
      "  [-1.186, 11.6%]\n",
      " [x9 < 1.000]\n",
      "  [0.083, 86.4%]\n",
      "  [2.742, 0.8%]\n",
      "Tree 4\n",
      "[x1 < -0.273]\n",
      " [1.609, 1.1%]\n",
      " [-0.02, 98.9%]\n"
     ]
    }
   ],
   "source": [
    "predictors = ['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10']\n",
    "response_2 = ['design2_y']\n",
    "treatment = ['treatment']\n",
    "for i in range(5):\n",
    "    print('Tree ' + str(i))\n",
    "    train_set, test_set = causal_train_test_split(fake_data, predictors, response_2, treatment, test_size = 0, estimation_size = 0.5)\n",
    "    tree_2  = causalTree(train_set, max_depth = 2, min_size = 25, criterion = 'causal')\n",
    "    print_tree(tree_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 0\n",
      "[x3 < 3.222]\n",
      " [1.763, 88.9%]\n",
      " [4.791, 11.1%]\n",
      "Tree 1\n",
      "[x20 < 2.657]\n",
      " [2.037, 97.3%]\n",
      " [2.526, 2.7%]\n",
      "Tree 2\n",
      "[x3 < 1.551]\n",
      " [x17 < 0.994]\n",
      "  [1.17, 62.5%]\n",
      "  [2.241, 1.8%]\n",
      " [x5 < 4.131]\n",
      "  [3.777, 31.9%]\n",
      "  [3.896, 3.8%]\n",
      "Tree 3\n",
      "[x3 < 1.542]\n",
      " [x4 < 0.968]\n",
      "  [1.162, 62.0%]\n",
      "  [3.376, 2.1%]\n",
      " [x13 < 7.345]\n",
      "  [3.589, 33.5%]\n",
      "  [4.428, 2.4%]\n",
      "Tree 4\n",
      "[x3 < 1.309]\n",
      " [x6 < 6.349]\n",
      "  [1.058, 55.6%]\n",
      "  [-0.724, 1.2%]\n",
      " [x5 < 4.398]\n",
      "  [3.539, 40.0%]\n",
      "  [4.161, 3.2%]\n"
     ]
    }
   ],
   "source": [
    "predictors = ['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10', 'x11','x12','x13','x14','x15','x16','x17','x18','x19','x20']\n",
    "response_3 = ['design3_y']\n",
    "treatment = ['treatment']\n",
    "for i in range(5):\n",
    "    print('Tree ' + str(i))\n",
    "    train_set, test_set = causal_train_test_split(fake_data, predictors, response_3, treatment, test_size = 0, estimation_size = 0.5)\n",
    "    tree_3  = causalTree(train_set, max_depth = 2, min_size = 25, criterion = 'causal')\n",
    "    print_tree(tree_3)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
